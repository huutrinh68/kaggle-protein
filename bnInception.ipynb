{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time \n",
    "import json \n",
    "import torch \n",
    "import random \n",
    "import warnings\n",
    "import torchvision\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from utils import *\n",
    "from data import HumanDataset\n",
    "from tqdm import tqdm \n",
    "from config import config\n",
    "from datetime import datetime\n",
    "from models.model import *\n",
    "from torch import nn,optim\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.metrics import f1_score\n",
    "# 1. set random seed\n",
    "random.seed(2050)\n",
    "np.random.seed(2050)\n",
    "torch.manual_seed(2050)\n",
    "torch.cuda.manual_seed_all(2050)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultConfigs(object):\n",
    "    train_data = \"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/train_all/\" # where is your train data\n",
    "    test_data = \"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/test_jpg/\"   # your test data\n",
    "    weights = \"./checkpoints/\"\n",
    "    best_models = \"./checkpoints/best_models/\"\n",
    "    submit = \"./submit/\"\n",
    "    model_name = \"bninception_bcelog\"\n",
    "    num_classes = 28\n",
    "    img_weight = 512\n",
    "    img_height = 512\n",
    "    channels = 4\n",
    "    lr = 0.03\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "\n",
    "config = DefaultConfigs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from imgaug import augmenters as iaa\n",
    "import random \n",
    "import pathlib\n",
    "\n",
    "# set random seed\n",
    "random.seed(2050)\n",
    "np.random.seed(2050)\n",
    "torch.manual_seed(2050)\n",
    "torch.cuda.manual_seed_all(2050)\n",
    "\n",
    "# create dataset class\n",
    "class HumanDataset(Dataset):\n",
    "    def __init__(self,images_df,base_path,augument=True,mode=\"train\"):\n",
    "        if not isinstance(base_path, pathlib.Path):\n",
    "            base_path = pathlib.Path(base_path)\n",
    "        self.images_df = images_df.copy()\n",
    "        self.augument = augument\n",
    "        self.images_df.Id = self.images_df.Id.apply(lambda x:base_path / x)\n",
    "        self.mlb = MultiLabelBinarizer(classes = np.arange(0,config.num_classes))\n",
    "        self.mlb.fit(np.arange(0,config.num_classes))\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_df)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        X = self.read_images(index)\n",
    "        if not self.mode == \"test\":\n",
    "            labels = np.array(list(map(int, self.images_df.iloc[index].Target.split(' '))))\n",
    "            y  = np.eye(config.num_classes,dtype=np.float)[labels].sum(axis=0)\n",
    "        else:\n",
    "            y = str(self.images_df.iloc[index].Id.absolute())\n",
    "        if self.augument:\n",
    "            X = self.augumentor(X)\n",
    "        #X = T.Compose([T.ToPILImage(),T.ToTensor(),T.Normalize([0.08069, 0.05258, 0.05487, 0.08282], [0.13704, 0.10145, 0.15313, 0.13814])])(X)\n",
    "        X = T.Compose([T.ToPILImage(),T.ToTensor()])(X)\n",
    "        return X.float(),y\n",
    "\n",
    "\n",
    "    def read_images(self,index):\n",
    "        row = self.images_df.iloc[index]\n",
    "        filename = str(row.Id.absolute())\n",
    "        #use only rgb channels\n",
    "        if config.channels == 4:\n",
    "            images = np.zeros(shape=(512,512,4))\n",
    "        else:\n",
    "            images = np.zeros(shape=(512,512,3))\n",
    "        r = np.array(Image.open(filename+\"_red.jpg\")) \n",
    "        g = np.array(Image.open(filename+\"_green.jpg\")) \n",
    "        b = np.array(Image.open(filename+\"_blue.jpg\")) \n",
    "        y = np.array(Image.open(filename+\"_yellow.jpg\")) \n",
    "        images[:,:,0] = r.astype(np.uint8) \n",
    "        images[:,:,1] = g.astype(np.uint8)\n",
    "        images[:,:,2] = b.astype(np.uint8)\n",
    "        if config.channels == 4:\n",
    "            images[:,:,3] = y.astype(np.uint8)\n",
    "        images = images.astype(np.uint8)\n",
    "        #images = np.stack(images,-1) \n",
    "        if config.img_height == 512:\n",
    "            return images\n",
    "        else:\n",
    "            return cv2.resize(images,(config.img_weight,config.img_height))\n",
    "\n",
    "    def augumentor(self,image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Affine(shear=(-16, 16)),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "                \n",
    "            ])], random_order=True)\n",
    "        \n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from pretrainedmodels.models import bninception\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "def get_net():\n",
    "    model = bninception(pretrained=\"imagenet\")\n",
    "    new_features = nn.Sequential(*list(model.children()))\n",
    "    # get the pre-trained weights of the first layer\n",
    "    pretrained_weights = new_features[0].weight\n",
    "\n",
    "    new_features[0] = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
    "    # For 4-channel weight should randomly initialized with Gaussian\n",
    "    #new_features[0].weight.data.normal_(0, 0.001)\n",
    "    # Y channel take B weight\n",
    "    new_features[0].weight.data[:, 1:4, :, :] = pretrained_weights\n",
    "    # For RGB it should be copied from pretrained weights\n",
    "    new_features[0].weight.data[:, :3, :, :] = pretrained_weights\n",
    "\n",
    "    # print(new_features[0].weight)\n",
    "    model.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    model.conv1_7x7_s2 = new_features[0]\n",
    "    model.last_linear = nn.Sequential(\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(1024, 28),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np \n",
    "from config import config\n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "from sklearn.metrics import f1_score\n",
    "from torch.autograd import Variable\n",
    "# save best model\n",
    "def save_checkpoint(state, is_best_loss,is_best_f1,fold):\n",
    "    filename = config.weights + config.model_name + os.sep +str(fold) + os.sep + \"checkpoint.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best_loss:\n",
    "        shutil.copyfile(filename,\"%s/%s_fold_%s_model_best_loss.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
    "    if is_best_f1:\n",
    "        shutil.copyfile(filename,\"%s/%s_fold_%s_model_best_f1.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
    "\n",
    "# evaluate meters\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "# print logger\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout  #stdout\n",
    "        self.file = None\n",
    "\n",
    "    def open(self, file, mode=None):\n",
    "        if mode is None: mode ='w'\n",
    "        self.file = open(file, mode)\n",
    "\n",
    "    def write(self, message, is_terminal=1, is_file=1 ):\n",
    "        if '\\r' in message: is_file=0\n",
    "\n",
    "        if is_terminal == 1:\n",
    "            self.terminal.write(message)\n",
    "            self.terminal.flush()\n",
    "            #time.sleep(1)\n",
    "\n",
    "        if is_file == 1:\n",
    "            self.file.write(message)\n",
    "            self.file.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25,gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''Focal loss.\n",
    "        Args:\n",
    "          x: (tensor) sized [N,D].\n",
    "          y: (tensor) sized [N,].\n",
    "        Return:\n",
    "          (tensor) focal loss.\n",
    "        '''\n",
    "        t = Variable(y).cuda()  # [N,20]\n",
    "\n",
    "        p = x.sigmoid()\n",
    "        pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
    "        w = self.alpha*t + (1-self.alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
    "        w = w * (1-pt).pow(self.gamma)\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)\n",
    "\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "       lr +=[ param_group['lr'] ]\n",
    "\n",
    "    #assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./logs/\"):\n",
    "    os.mkdir(\"./logs/\")\n",
    "\n",
    "log = Logger()\n",
    "log.open(\"logs/%s_log_train.txt\"%config.model_name,mode=\"a\")\n",
    "log.write(\"\\n----------------------------------------------- [START %s] %s\\n\\n\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '-' * 51))\n",
    "log.write('                           |------------ Train -------------|----------- Valid -------------|----------Best Results---------|------------|\\n')\n",
    "log.write('mode     iter     epoch    |         loss   f1_macro        |         loss   f1_macro       |         loss   f1_macro       | time       |\\n')\n",
    "log.write('-------------------------------------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "def train(train_loader,model,criterion,optimizer,epoch,valid_loss,best_results,start):\n",
    "    losses = AverageMeter()\n",
    "    f1 = AverageMeter()\n",
    "    model.train()\n",
    "    for i,(images,target) in enumerate(train_loader):\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output,target)\n",
    "        losses.update(loss.item(),images.size(0))\n",
    "        \n",
    "        f1_batch = f1_score(target.cpu.numpy(),output.sigmoid().cpu() > 0.15,average='macro')\n",
    "        f1.update(f1_batch,images.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('\\r',end='',flush=True)\n",
    "        message = '%s %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                \"train\", i/len(train_loader) + epoch, epoch,\n",
    "                losses.avg, f1.avg, \n",
    "                valid_loss[0], valid_loss[1], \n",
    "                str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                time_to_str((timer() - start),'min'))\n",
    "        print(message , end='',flush=True)\n",
    "    log.write(\"\\n\")\n",
    "    #log.write(message)\n",
    "    #log.write(\"\\n\")\n",
    "    return [losses.avg,f1.avg]\n",
    "\n",
    "# 2. evaluate fuunction\n",
    "def evaluate(val_loader,model,criterion,epoch,train_loss,best_results,start):\n",
    "    # only meter loss and f1 score\n",
    "    losses = AverageMeter()\n",
    "    f1 = AverageMeter()\n",
    "    # switch mode for evaluation\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images,target) in enumerate(val_loader):\n",
    "            images_var = images.cuda(non_blocking=True)\n",
    "            target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
    "            #image_var = Variable(images).cuda()\n",
    "            #target = Variable(torch.from_numpy(np.array(target)).long()).cuda()\n",
    "            output = model(images_var)\n",
    "            loss = criterion(output,target)\n",
    "            losses.update(loss.item(),images_var.size(0))\n",
    "            f1_batch = f1_score(target.cpu.numpy(),output.sigmoid().cpu().data.numpy() > 0.15,average='macro')\n",
    "            f1.update(f1_batch,images_var.size(0))\n",
    "            print('\\r',end='',flush=True)\n",
    "            message = '%s   %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                    \"val\", i/len(val_loader) + epoch, epoch,                    \n",
    "                    train_loss[0], train_loss[1], \n",
    "                    losses.avg, f1.avg,\n",
    "                    str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                    time_to_str((timer() - start),'min'))\n",
    "\n",
    "            print(message, end='',flush=True)\n",
    "        log.write(\"\\n\")\n",
    "        #log.write(message)\n",
    "        #log.write(\"\\n\")\n",
    "        \n",
    "    return [losses.avg,f1.avg]\n",
    "\n",
    "# 3. test model on public dataset and save the probability matrix\n",
    "def test(test_loader,model,folds):\n",
    "    sample_submission_df = pd.read_csv(\"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/sample_submission.csv\")\n",
    "    #3.1 confirm the model converted to cuda\n",
    "    filenames,labels ,submissions= [],[],[]\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    submit_results = []\n",
    "    for i,(input,filepath) in enumerate(tqdm(test_loader)):\n",
    "        #3.2 change everything to cuda and get only basename\n",
    "        filepath = [os.path.basename(x) for x in filepath]\n",
    "        with torch.no_grad():\n",
    "            image_var = input.cuda(non_blocking=True)\n",
    "            y_pred = model(image_var)\n",
    "            label = y_pred.sigmoid().cpu().data.numpy()\n",
    "            #print(label > 0.5)\n",
    "           \n",
    "            labels.append(label > 0.15)\n",
    "            filenames.append(filepath)\n",
    "\n",
    "    for row in np.concatenate(labels):\n",
    "        subrow = ' '.join(list([str(i) for i in np.nonzero(row)[0]]))\n",
    "        submissions.append(subrow)\n",
    "    sample_submission_df['Predicted'] = submissions\n",
    "    sample_submission_df.to_csv('./submit/%s_bestloss_submission.csv'%config.model_name, index=None)\n",
    "\n",
    "# 4. main function\n",
    "def main():\n",
    "    fold = 0\n",
    "    # 4.1 mkdirs\n",
    "    if not os.path.exists(config.submit):\n",
    "        os.makedirs(config.submit)\n",
    "    if not os.path.exists(config.weights + config.model_name + os.sep +str(fold)):\n",
    "        os.makedirs(config.weights + config.model_name + os.sep +str(fold))\n",
    "    if not os.path.exists(config.best_models):\n",
    "        os.mkdir(config.best_models)\n",
    "    if not os.path.exists(\"./logs/\"):\n",
    "        os.mkdir(\"./logs/\")\n",
    "    \n",
    "    # 4.2 get model\n",
    "    model = get_net()\n",
    "    model.cuda()\n",
    "\n",
    "    # criterion\n",
    "    optimizer = optim.SGD(model.parameters(),lr = config.lr,momentum=0.9,weight_decay=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "    #criterion = FocalLoss().cuda()\n",
    "    #criterion = F1Loss().cuda()\n",
    "    start_epoch = 0\n",
    "    best_loss = 999\n",
    "    best_f1 = 0\n",
    "    best_results = [np.inf,0]\n",
    "    val_metrics = [np.inf,0]\n",
    "    resume = False\n",
    "    df1 = pd.read_csv(\"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/train.csv\")\n",
    "    df2 = pd.read_csv(\"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/external_data/img/train.csv\")\n",
    "    df = [df1, df2]\n",
    "    all_files = pd.concat(df)\n",
    "#     all_files = pd.read_csv(\"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/train.csv\")\n",
    "    test_files = pd.read_csv(\"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/sample_submission.csv\")\n",
    "    train_data_list,val_data_list = train_test_split(all_files,test_size = 0.13,random_state = 2050)\n",
    "\n",
    "    # load dataset\n",
    "    train_gen = HumanDataset(train_data_list,config.train_data,mode=\"train\")\n",
    "    train_loader = DataLoader(train_gen,batch_size=config.batch_size,shuffle=True,pin_memory=True,num_workers=4)\n",
    "\n",
    "    val_gen = HumanDataset(val_data_list,config.train_data,augument=False,mode=\"train\")\n",
    "    val_loader = DataLoader(val_gen,batch_size=config.batch_size,shuffle=False,pin_memory=True,num_workers=4)\n",
    "\n",
    "    test_gen = HumanDataset(test_files,config.test_data,augument=False,mode=\"test\")\n",
    "    test_loader = DataLoader(test_gen,1,shuffle=False,pin_memory=True,num_workers=4)\n",
    "\n",
    "    scheduler = lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)\n",
    "    start = timer()\n",
    "\n",
    "    #train\n",
    "    for epoch in range(0,config.epochs):\n",
    "        scheduler.step(epoch)\n",
    "        # train\n",
    "        lr = get_learning_rate(optimizer)\n",
    "        train_metrics = train(train_loader,model,criterion,optimizer,epoch,val_metrics,best_results,start)\n",
    "        # val\n",
    "        val_metrics = evaluate(val_loader,model,criterion,epoch,train_metrics,best_results,start)\n",
    "        # check results \n",
    "        is_best_loss = val_metrics[0] < best_results[0]\n",
    "        best_results[0] = min(val_metrics[0],best_results[0])\n",
    "        is_best_f1 = val_metrics[1] > best_results[1]\n",
    "        best_results[1] = max(val_metrics[1],best_results[1])   \n",
    "        # save model\n",
    "        save_checkpoint({\n",
    "                    \"epoch\":epoch + 1,\n",
    "                    \"model_name\":config.model_name,\n",
    "                    \"state_dict\":model.state_dict(),\n",
    "                    \"best_loss\":best_results[0],\n",
    "                    \"optimizer\":optimizer.state_dict(),\n",
    "                    \"fold\":fold,\n",
    "                    \"best_f1\":best_results[1],\n",
    "        },is_best_loss,is_best_f1,fold)\n",
    "        # print logs\n",
    "        print('\\r',end='',flush=True)\n",
    "        log.write('%s  %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                \"best\", epoch, epoch,                    \n",
    "                train_metrics[0], train_metrics[1], \n",
    "                val_metrics[0], val_metrics[1],\n",
    "                str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                time_to_str((timer() - start),'min'))\n",
    "            )\n",
    "        log.write(\"\\n\")\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "    best_model = torch.load(\"%s/%s_fold_%s_model_best_loss.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
    "    #best_model = torch.load(\"checkpoints/bninception_bcelog/0/checkpoint.pth.tar\")\n",
    "    model.load_state_dict(best_model[\"state_dict\"])\n",
    "    test(test_loader,model,fold)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "in_path = \"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/external_data/img/train_resize/\"\n",
    "out_path = \"/media/trinhnh1/3A08638408633DCF/kaggle/human-protein/input/train_all/\"\n",
    "\n",
    "all_files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "for file in tqdm(all_files):\n",
    "    img = Image.open(path+file).convert('L')\n",
    "    img.save(out_path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28, 25, 30, ..., 23, 44, 13],\n",
       "       [26, 22, 33, ..., 15, 26, 21],\n",
       "       [26, 22, 36, ..., 14, 23, 32],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  3,  4,  7],\n",
       "       [ 0,  0,  0, ...,  5,  5,  5],\n",
       "       [ 0,  0,  0, ...,  6,  5,  4]], dtype=uint8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
